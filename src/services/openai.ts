// AI Court - OpenAI API Service (via Cloudflare Functions in prod, direct in dev)
import type { Message, RoleType, LegalCategory, CaseType } from '@/types'

const API_BASE = import.meta.env.VITE_API_BASE || ''

// System prompts for each role
const JUDGE_PROMPT = `ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ì›ì˜ ê³µì •í•˜ê³  ê¶Œìœ„ ìžˆëŠ” íŒì‚¬ìž…ë‹ˆë‹¤.
ì—­í• :
- ìž¬íŒì„ ì§„í–‰í•˜ê³  ì–‘ì¸¡ ì£¼ìž¥ì„ ê³µì •í•˜ê²Œ ë“£ìŠµë‹ˆë‹¤
- ë²•ë¥ ì  ê·¼ê±°ì— ë”°ë¼ íŒë‹¨í•©ë‹ˆë‹¤
- ëª…í™•í•˜ê³  ê¶Œìœ„ ìžˆëŠ” ì–´ì¡°ë¡œ ë°œì–¸í•©ë‹ˆë‹¤
- í•œêµ­ ë¯¼ë²•, í˜•ë²•, ë¯¼ì‚¬ì†Œì†¡ë²•ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤
- ë°œì–¸ì€ ê°„ê²°í•˜ê²Œ 2-4ë¬¸ìž¥ìœ¼ë¡œ í•©ë‹ˆë‹¤
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
ì£¼ì˜: ì´ ì„œë¹„ìŠ¤ëŠ” ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ì´ë©° ì‹¤ì œ ë²•ë¥  ìžë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.`

const PROSECUTOR_PROMPT = `ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ìœ ëŠ¥í•œ ê²€ì‚¬/ì›ê³  ì¸¡ ë³€í˜¸ì‚¬ìž…ë‹ˆë‹¤.
ì—­í• :
- ì›ê³ (ê³ ì†Œì¸) ì¸¡ì˜ ìž…ìž¥ì„ ê°•ë ¥í•˜ê²Œ ë³€í˜¸í•©ë‹ˆë‹¤
- ë²•ì  ê·¼ê±°ì™€ ì¦ê±°ë¥¼ ë“¤ì–´ ì£¼ìž¥í•©ë‹ˆë‹¤
- ìƒëŒ€ë°© ì£¼ìž¥ì˜ í—ˆì ì„ ë‚ ì¹´ë¡­ê²Œ ì§€ì í•©ë‹ˆë‹¤
- ê³µê²©ì ì´ì§€ë§Œ ë²•ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ë…¼ë¦¬ë¥¼ íŽ¼ì¹©ë‹ˆë‹¤
- ë°œì–¸ì€ 2-4ë¬¸ìž¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•©ë‹ˆë‹¤
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤`

const DEFENSE_PROMPT = `ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ìœ ëŠ¥í•œ í”¼ê³  ì¸¡ ë³€í˜¸ì‚¬ìž…ë‹ˆë‹¤.
ì—­í• :
- í”¼ê³ (í”¼ê³ ì†Œì¸) ì¸¡ì˜ ìž…ìž¥ì„ ê°•ë ¥í•˜ê²Œ ë³€í˜¸í•©ë‹ˆë‹¤
- ë²•ì  ê·¼ê±°ì™€ ë°˜ì¦ì„ ë“¤ì–´ ë°©ì–´í•©ë‹ˆë‹¤
- ê²€ì‚¬/ì›ê³  ì¸¡ ì£¼ìž¥ì˜ ë…¼ë¦¬ì  í—ˆì ì„ ë°˜ë°•í•©ë‹ˆë‹¤
- ì˜ë¢°ì¸ì—ê²Œ ìœ ë¦¬í•œ ë²•ì  í•´ì„ì„ ì œì‹œí•©ë‹ˆë‹¤
- ë°œì–¸ì€ 2-4ë¬¸ìž¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•©ë‹ˆë‹¤
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤`

export interface StreamChunk {
  role: RoleType
  content: string
  done: boolean
}

type StreamCallback = (chunk: StreamChunk) => void

// Quick consultation - single GPT call
export async function quickConsult(
  question: string,
  category: LegalCategory,
  onChunk: StreamCallback
): Promise<void> {
  const systemPrompt = `ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
ì‚¬ìš©ìžì˜ ë²•ë¥  ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
ì¹´í…Œê³ ë¦¬: ${category}
ë‹µë³€ í˜•ì‹:
1. í•µì‹¬ ë²•ë¥  ìš”ì  (2-3ì¤„)
2. ê´€ë ¨ ë²•ì¡°í•­ (ìžˆë‹¤ë©´)
3. ê¶Œê³  í–‰ë™ (1-2ì¤„)
âš ï¸ ì´ ë‹µë³€ì€ ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ì´ë©° ì‹¤ì œ ë²•ë¥  ìžë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`

  await streamOpenAI([
    { role: 'system', content: systemPrompt },
    { role: 'user', content: question }
  ], (content, done) => {
    onChunk({ role: 'judge', content, done })
  })
}

// Trial simulation - sequential multi-role calls
export async function runTrialRound(
  caseDescription: string,
  caseType: CaseType,
  round: number,
  previousMessages: Message[],
  onChunk: StreamCallback
): Promise<void> {
  const context = previousMessages
    .map(m => {
      const roleLabel = m.role === 'judge' ? 'íŒì‚¬' : m.role === 'prosecutor' ? 'ê²€ì‚¬/ì›ê³ ' : 'ë³€í˜¸ì‚¬/í”¼ê³ '
      return `[${roleLabel}]: ${m.content}`
    })
    .join('\n')

  const caseContext = `ì‚¬ê±´ ìœ í˜•: ${caseType === 'civil' ? 'ë¯¼ì‚¬' : 'í˜•ì‚¬'}
ì‚¬ê±´ ë‚´ìš©: ${caseDescription}
í˜„ìž¬ ë¼ìš´ë“œ: ${round}
ì´ì „ ë°œì–¸:
${context}`

  // Round structure:
  // 1: Judge opens court
  // 2: Prosecutor opening statement
  // 3: Defense opening statement
  // 4: Judge mid-trial questions
  // 5: Prosecutor rebuttal
  // 6: Defense rebuttal
  // 7: Judge final verdict
  if (round === 1) {
    await streamOpenAI([
      { role: 'system', content: JUDGE_PROMPT },
      { role: 'user', content: `ë‹¤ìŒ ì‚¬ê±´ì˜ ìž¬íŒì„ ì‹œìž‘í•©ë‹ˆë‹¤. ê°œì •ì„ ì„ ì–¸í•˜ê³  ì–‘ì¸¡ì— ì£¼ìž¥ ê¸°íšŒë¥¼ ì£¼ì„¸ìš”.\n${caseContext}` }
    ], (content, done) => onChunk({ role: 'judge', content, done }))
  } else if (round === 2 || round === 5) {
    await streamOpenAI([
      { role: 'system', content: PROSECUTOR_PROMPT },
      { role: 'user', content: `ë‹¤ìŒ ì‚¬ê±´ì—ì„œ ì›ê³ /ê²€ì‚¬ ì¸¡ ì£¼ìž¥ì„ íŽ¼ì¹˜ì„¸ìš”. êµ¬ì²´ì ì¸ ë²•ì  ê·¼ê±°ë¥¼ ë“¤ì–´ ì£¼ìž¥í•˜ì„¸ìš”.\n${caseContext}` }
    ], (content, done) => onChunk({ role: 'prosecutor', content, done }))
  } else if (round === 3 || round === 6) {
    await streamOpenAI([
      { role: 'system', content: DEFENSE_PROMPT },
      { role: 'user', content: `ë‹¤ìŒ ì‚¬ê±´ì—ì„œ í”¼ê³ /ë³€í˜¸ì¸ ì¸¡ ë°˜ë°•ì„ íŽ¼ì¹˜ì„¸ìš”. ê²€ì‚¬ ì£¼ìž¥ì˜ í—ˆì ì„ ì§€ì í•˜ê³  ë°©ì–´í•˜ì„¸ìš”.\n${caseContext}` }
    ], (content, done) => onChunk({ role: 'defense', content, done }))
  } else if (round === 4) {
    await streamOpenAI([
      { role: 'system', content: JUDGE_PROMPT },
      { role: 'user', content: `ì–‘ì¸¡ ì£¼ìž¥ì„ ë“¤ì—ˆìŠµë‹ˆë‹¤. í•µì‹¬ ìŸì ì„ ì •ë¦¬í•˜ê³  ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ì„ ì§ˆë¬¸í•˜ì„¸ìš”.\n${caseContext}` }
    ], (content, done) => onChunk({ role: 'judge', content, done }))
  } else {
    // Round 7: Final verdict
    await streamOpenAI([
      { role: 'system', content: JUDGE_PROMPT },
      { role: 'user', content: `ëª¨ë“  ì£¼ìž¥ì„ ë“¤ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… íŒê²°ì„ ë‚´ë ¤ì£¼ì„¸ìš”.\ní˜•ì‹: [ì£¼ë¬¸] íŒê²° ë‚´ìš© / [ì´ìœ ] ë²•ì  ê·¼ê±° / [ê¶Œê³ ì‚¬í•­] í–¥í›„ ì¡°ì¹˜\n${caseContext}` }
    ], (content, done) => onChunk({ role: 'judge', content, done }))
  }
}

// Document analysis
export async function analyzeDocument(
  documentText: string,
  userSide: 'plaintiff' | 'defendant',
  onChunk: StreamCallback
): Promise<void> {
  const sideLabel = userSide === 'plaintiff' ? 'ì›ê³ (ê³ ì†Œì¸)' : 'í”¼ê³ (í”¼ê³ ì†Œì¸)'
  
  const systemPrompt = `ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ íŒ€ìž…ë‹ˆë‹¤. 
ì œì¶œëœ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ${sideLabel} ìž…ìž¥ì—ì„œ ìž¬íŒ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
ë¶„ì„ í˜•ì‹:
ðŸ“‹ ë¬¸ì„œ ìš”ì•½
âš”ï¸ í•µì‹¬ ìŸì 
ðŸ”´ ë¶ˆë¦¬í•œ ì 
ðŸ”µ ìœ ë¦¬í•œ ì   
âš–ï¸ ì˜ˆìƒ íŒê²° ë°©í–¥
ðŸ’¡ ê¶Œê³  ì „ëžµ
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`

  await streamOpenAI([
    { role: 'system', content: systemPrompt },
    { role: 'user', content: `ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n${documentText}` }
  ], (content, done) => {
    onChunk({ role: 'judge', content, done })
  })
}

// Core streaming function
async function streamOpenAI(
  messages: Array<{ role: string; content: string }>,
  onChunk: (content: string, done: boolean) => void
): Promise<void> {
  const apiKey = import.meta.env.VITE_OPENAI_API_KEY

  // In production, use Cloudflare Function; in dev, call directly
  const endpoint = API_BASE 
    ? `${API_BASE}/api/chat` 
    : 'https://api.openai.com/v1/chat/completions'

  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
  }
  
  if (!API_BASE && apiKey) {
    headers['Authorization'] = `Bearer ${apiKey}`
  }

  const response = await fetch(endpoint, {
    method: 'POST',
    headers,
    body: JSON.stringify({
      model: 'gpt-4o',
      messages,
      stream: true,
      max_tokens: 600,
      temperature: 0.8,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`API Error: ${response.status} - ${error}`)
  }

  const reader = response.body?.getReader()
  if (!reader) throw new Error('No response body')

  const decoder = new TextDecoder()
  let buffer = ''

  while (true) {
    const { done, value } = await reader.read()
    if (done) break

    buffer += decoder.decode(value, { stream: true })
    const lines = buffer.split('\n')
    buffer = lines.pop() || ''

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6).trim()
        if (data === '[DONE]') {
          onChunk('', true)
          return
        }
        try {
          const parsed = JSON.parse(data)
          const content = parsed.choices?.[0]?.delta?.content || ''
          if (content) onChunk(content, false)
        } catch {
          // Skip malformed JSON
        }
      }
    }
  }
  onChunk('', true)
}
